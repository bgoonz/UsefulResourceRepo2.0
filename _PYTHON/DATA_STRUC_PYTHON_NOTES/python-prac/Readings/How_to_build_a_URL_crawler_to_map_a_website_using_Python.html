<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <meta charset="utf-8" />
    <meta name="generator" content="pandoc" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, user-scalable=yes"
    />
    <title>How_to_build_a_URL_crawler_to_map_a_website_using_Python</title>
    <style type="text/css">
      code {
        white-space: pre-wrap;
      }
      span.smallcaps {
        font-variant: small-caps;
      }
      span.underline {
        text-decoration: underline;
      }
      div.column {
        display: inline-block;
        vertical-align: top;
        width: 50%;
      }
    </style>
  </head>
  <body>
    <h1 id="how-to-build-a-url-crawler-to-map-a-website-using-python">
      How to build a URL crawler to map a website using Python
    </h1>
    <blockquote>
      <p>
        by Ahad Sheriff How to build a URL crawler to map a website using
        PythonA simple project for learning the fundamentals of web
        scrapingBefore we start, let’s make sure we understand what web scraping
        is: Web scraping is the process of extracting data from websites to
        present it in
      </p>
    </blockquote>
    <p>by Ahad Sheriff</p>
    <h4 id="a-simple-project-for-learning-the-fundamentals-of-web-scraping">
      A simple project for learning the fundamentals of web scraping
    </h4>
    <p>
      <img
        src="https://cdn-media-1.freecodecamp.org/images/1*ZxUfhtbRROKqcBqyfT8plA.jpeg"
      />
    </p>
    <p>Before we start, let’s make sure we understand what web scraping is:</p>
    <blockquote>
      <p>
        <strong>Web scraping</strong> is the process of extracting data from
        websites to present it in a format users can easily make sense of.
      </p>
    </blockquote>
    <p>
      In this tutorial, I want to demonstrate how easy it is to build a simple
      URL crawler in Python that you can use to map websites. While this program
      is relatively simple, it can provide a great introduction to the
      fundamentals of web scraping and automation. We will be focusing on
      recursively extracting links from web pages, but the same ideas can be
      applied to a myriad of other solutions.
    </p>
    <p>Our program will work like this:</p>
    <ol type="1">
      <li>Visit a web page</li>
      <li>
        Scrape all unique URL’s found on the webpage and add them to a queue
      </li>
      <li>Recursively process URL’s one by one until we exhaust the queue</li>
      <li>Print results</li>
    </ol>
    <h3 id="first-things-first">First Things First</h3>
    <p>
      The first thing we should do is import all the necessary libraries. We
      will be using
      <a href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a
      >, <a href="http://docs.python-requests.org/en/master/">requests</a>, and
      <a href="https://docs.python.org/3/library/urllib.html">urllib</a> for web
      scraping.
    </p>
    <pre><code>from bs4 import BeautifulSoupimport requestsimport requests.exceptionsfrom urllib.parse import urlsplitfrom urllib.parse import urlparsefrom collections import deque</code></pre>
    <p>
      Next, we need to select a URL to start crawling from. While you can choose
      any webpage with HTML links, I recommend using
      <a href="https://scrapethissite.com/">ScrapeThisSite</a>. It is a safe
      sandbox that you can crawl without getting in trouble.
    </p>
    <pre><code>url = “https://scrapethissite.com&quot;</code></pre>
    <p>
      Next, we are going to need to create a new
      <a
        href="https://docs.python.org/3.3/library/collections.html#collections.deque"
        >deque</a
      >
      object so that we can easily add newly found links and remove them once we
      are finished processing them. Pre-populate the deque with your
      <code>url</code> variable:
    </p>
    <pre><code># a queue of urls to be crawled nextnew_urls = deque([url])</code></pre>
    <p>
      We can then use a
      <a
        href="https://docs.python.org/3.3/library/stdtypes.html?highlight=set#set"
        >set</a
      >
      to store unique URL’s once they have been processed:
    </p>
    <pre><code># a set of urls that we have already processed processed_urls = set()</code></pre>
    <p>
      We also want to keep track of local (same domain as the target), foreign
      (different domain as the target), and broken URLs:
    </p>
    <pre><code># a set of domains inside the target websitelocal_urls = set()

# a set of domains outside the target websiteforeign_urls = set()

# a set of broken urlsbroken_urls = set()</code></pre>
    <h3 id="time-to-crawl">Time To Crawl</h3>
    <p>
      With all that in place, we can now start writing the actual code to crawl
      the website.
    </p>
    <p>
      We want to look at each URL in the queue, see if there are any additional
      URL’s within that page and add each one to the end of the queue until
      there are none left. As soon as we finish scraping a URL, we will remove
      it from the queue and add it to the <code>processed_urls</code> set for
      later use.
    </p>
    <pre><code># process urls one by one until we exhaust the queuewhile len(new_urls):    # move url from the queue to processed url set    url = new_urls.popleft()    processed_urls.add(url)    # print the current url    print(“Processing %s&quot; % url)</code></pre>
    <p>
      Next, add an exception to catch any broken web pages and add them to the
      <code>broken_urls</code> set for later use:
    </p>
    <pre><code>try:    response = requests.get(url)

except(requests.exceptions.MissingSchema, requests.exceptions.ConnectionError, requests.exceptions.InvalidURL, requests.exceptions.InvalidSchema):    # add broken urls to it’s own set, then continue    broken_urls.add(url)    continue</code></pre>
    <p>
      We then need to get the base URL of the webpage so that we can easily
      differentiate local and foreign addresses:
    </p>
    <pre><code># extract base url to resolve relative linksparts = urlsplit(url)base = “{0.netloc}&quot;.format(parts)strip_base = base.replace(“www.&quot;, “&quot;)base_url = “{0.scheme}://{0.netloc}&quot;.format(parts)path = url[:url.rfind(‘/’)+1] if ‘/’ in parts.path else url</code></pre>
    <p>Initialize BeautifulSoup to process the HTML document:</p>
    <pre><code>soup = BeautifulSoup(response.text, “lxml&quot;)</code></pre>
    <p>
      Now scrape the web page for all links and sort add them to their
      corresponding set:
    </p>
    <pre><code>for link in soup.find_all(‘a’):    # extract link url from the anchor    anchor = link.attrs[“href&quot;] if “href&quot; in link.attrs else ‘’

if anchor.startswith(‘/’):        local_link = base_url + anchor        local_urls.add(local_link)    elif strip_base in anchor:        local_urls.add(anchor)    elif not anchor.startswith(‘http’):        local_link = path + anchor        local_urls.add(local_link)    else:        foreign_urls.add(anchor)</code></pre>
    <p>
      Since I want to limit my crawler to local addresses only, I add the
      following to add new URLs to our queue:
    </p>
    <pre><code>for i in local_urls:    if not i in new_urls and not i in processed_urls:        new_urls.append(i)</code></pre>
    <p>If you want to crawl all URLs use:</p>
    <pre><code>if not link in new_urls and not link in processed_urls:    new_urls.append(link)</code></pre>
    <p>
      <strong><em>Warning:</em></strong>
      <em
        >The way the program currently works, crawling foreign URL’s will take
        a</em
      >
      <strong><em>VERY</em></strong>
      <em
        >long time. You could possibly get into trouble for scraping websites
        without permission.</em
      >
      <strong><em>Use at your own risk!</em></strong>
    </p>
    <p>
      <img
        src="https://cdn-media-1.freecodecamp.org/images/1*Y5DwSdLwAIGOWuuyvp1HnA.png"
      />
    </p>
    <p>Sample output</p>
    <p>Here is all my code:</p>
    <p>
      And that should be it. You have just created a simple tool to crawl a
      website and map all URLs found!
    </p>
  </body>
</html>
